{"cells":[{"cell_type":"code","source":["!pip install keras_nlp"],"metadata":{"id":"sT_PSdaobpD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","import tensorflow as tf\n","import keras_nlp\n","import numpy as np\n","from transformers import AutoTokenizer\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","np.random.seed(2)\n","tokenizer = AutoTokenizer.from_pretrained('t5-base', bos_token=\"<start>\")\n","\n","\n","class TransformerEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, embedding_dim, max_seq_len):\n","        super(TransformerEmbedding, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.positional_encoding = keras_nlp.layers.SinePositionEncoding()\n","\n","    def call(self, sequences):\n","        embeddings = self.embedding(sequences)\n","        positional_encoding = self.positional_encoding(embeddings)\n","        outputs = embeddings + positional_encoding\n","        return outputs\n","\n","    def get_weights(self):\n","        return self.embedding.get_weights()\n","\n","    def set_weights(self, weights):\n","        self.embedding.set_weights(weights)\n","\n","\n","class FeedForward(tf.keras.layers.Layer):\n","    def __init__(self, dModel):\n","        super(FeedForward, self).__init__()\n","        self.l1 = tf.keras.layers.Dense(dModel * 4, activation='relu')\n","        self.l2 = tf.keras.layers.Dense(dModel)\n","\n","    def call(self, x, *args, **kwargs):\n","        x = self.l1(x)\n","        x = self.l2(x)\n","        return x\n","\n","    def get_weights(self):\n","        return self.l1.get_weights() + self.l2.get_weights()\n","\n","    def set_weights(self, weights):\n","        l1_weights = weights[:2]\n","        l2_weights = weights[2:]\n","        self.l1.set_weights(l1_weights)\n","        self.l2.set_weights(l2_weights)\n","\n","\n","class EncoderBlock(tf.keras.layers.Layer):\n","    def __init__(self, dModel, num_heads):\n","        super(EncoderBlock, self).__init__()\n","        self.dModel = dModel\n","        self.num_heads = num_heads\n","        self.MhA = tf.keras.layers.MultiHeadAttention(num_heads, dModel // num_heads)\n","        self.LayerNorm1 = tf.keras.layers.LayerNormalization()\n","        self.LayerNorm2 = tf.keras.layers.LayerNormalization()\n","        self.Add = tf.keras.layers.Add()\n","        self.FeedForward = FeedForward(dModel)\n","\n","    def call(self, x, *args, **kwargs):\n","        skip = x\n","        x = self.MhA(key=x, query=x, value=x)\n","        x = self.Add([x, skip])\n","        x = self.LayerNorm1(x)\n","        skip = x\n","        x = self.FeedForward(x)\n","        x = self.Add([x, skip])\n","        x = self.LayerNorm2(x)\n","        return x\n","\n","    def get_weights(self):\n","        mha_weights=self.MhA.get_weights()\n","\n","        weights= mha_weights+ self.LayerNorm1.get_weights() + \\\n","            self.LayerNorm2.get_weights() + self.FeedForward.get_weights()\n","        return weights\n","\n","    def set_weights(self, weights):\n","        mhA_weights = weights[:8]\n","        lNorm1_weights = weights[8:10]\n","        lNorm2_weights = weights[10:12]\n","        ff_weights = weights[12:]\n","        self.MhA.set_weights(mhA_weights)\n","        self.LayerNorm1.set_weights(lNorm1_weights)\n","        self.LayerNorm2.set_weights(lNorm2_weights)\n","        self.FeedForward.set_weights(ff_weights)\n","\n","\n","class DecoderBlock(tf.keras.layers.Layer):\n","    def __init__(self, dModel, num_heads):\n","        super(DecoderBlock, self).__init__()\n","        self.dModel = dModel\n","        self.num_heads = num_heads\n","        self.MhA = tf.keras.layers.MultiHeadAttention(num_heads, dModel // num_heads)\n","        self.MMhA= tf.keras.layers.MultiHeadAttention(num_heads, dModel // num_heads)\n","        self.LayerNorm1 = tf.keras.layers.LayerNormalization()\n","        self.LayerNorm2 = tf.keras.layers.LayerNormalization()\n","        self.LayerNorm3 = tf.keras.layers.LayerNormalization()\n","        self.Add = tf.keras.layers.Add()\n","        self.FeedForward = FeedForward(dModel)\n","\n","    def call(self, x, encoder_out, *args, **kwargs):\n","        skip = x\n","        x = self.MMhA(query=x, value=x, key=x, use_causal_mask=True)\n","        x = self.Add([x, skip])\n","        x = self.LayerNorm1(x)\n","        skip = x\n","        x = self.MhA(query=x, key=encoder_out, value=encoder_out)\n","        x = self.Add([x, skip])\n","        x = self.LayerNorm2(x)\n","        skip = x\n","        x = self.FeedForward(x)\n","        x = self.Add([x, skip])\n","        x = self.LayerNorm3(x)\n","        return x\n","\n","    def get_weights(self):\n","        return self.MMhA.get_weights() + self.MhA.get_weights() + \\\n","               self.LayerNorm1.get_weights() + self.LayerNorm2.get_weights() + \\\n","               self.LayerNorm3.get_weights() + self.FeedForward.get_weights()\n","\n","    def set_weights(self, weights):\n","        mmhA_weights = weights[:8]\n","        mhA_weights = weights[8:16]\n","        lNorm1_weights = weights[16:18]\n","        lNorm2_weights = weights[18:20]\n","        lNorm3_weights = weights[20:22]\n","        ff_weights = weights[22:]\n","        self.MMhA.set_weights(mmhA_weights)\n","        self.MhA.set_weights(mhA_weights)\n","        self.LayerNorm1.set_weights(lNorm1_weights)\n","        self.LayerNorm2.set_weights(lNorm2_weights)\n","        self.LayerNorm3.set_weights(lNorm3_weights)\n","        self.FeedForward.set_weights(ff_weights)\n","\n","\n","class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_blocks, vocab_size, dModel, num_heads):\n","        super(Decoder, self).__init__()\n","        self.num_blocks = num_blocks\n","        self.blocks = [DecoderBlock(dModel, num_heads) for _ in range(num_blocks)]\n","        self.linear= tf.keras.layers.Dense(vocab_size,activation='softmax')\n","\n","    def call(self, x, encoder_out ,*args, **kwargs):\n","        for block in self.blocks:\n","            x = block(x, encoder_out)\n","        x = self.linear(x)\n","        return x\n","\n","    def get_weights(self):\n","        weights = []\n","        for block in self.blocks:\n","            weights.extend(block.get_weights())\n","        return weights\n","\n","    def set_weights(self, weights):\n","        for block in self.blocks:\n","            block.set_weights(weights[:len(block.get_weights())])\n","            weights = weights[len(block.get_weights()):]\n","\n","\n","class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_blocks, dModel, num_heads):\n","        super(Encoder, self).__init__()\n","        self.num_blocks = num_blocks\n","        self.blocks = [EncoderBlock(dModel, num_heads) for _ in range(num_blocks)]\n","\n","    def call(self, x, *args, **kwargs):\n","        for block in self.blocks:\n","            x = block(x)\n","        return x\n","\n","    def get_weights(self):\n","        weights = []\n","        for block in self.blocks:\n","            weights.extend(block.get_weights())\n","        return weights\n","\n","    def set_weights(self, weights):\n","        for block in self.blocks:\n","            block.set_weights(weights[:len(block.get_weights())])\n","            weights = weights[len(block.get_weights()):]\n"],"metadata":{"id":"in-1fFsSaIen"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ff-P7cxNBzyW"},"outputs":[],"source":["\n","class Transformer(tf.keras.Model):\n","    def __init__(self, embedding_layer, encoder, decoder, optimizer):\n","        super(Transformer, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.embedding_layer = embedding_layer\n","        self.optimizer = optimizer\n","\n","    def save(self, filepath):\n","        weights = self.get_weights()\n","        with open(filepath, 'wb') as f:\n","            pickle.dump(weights, f)\n","\n","    def load(self, filepath):\n","        with open(filepath, 'rb') as f:\n","            weights = pickle.load(f)\n","        self.set_weights(weights)\n","\n","    def get_weights(self):\n","        encoder_weights = self.encoder.get_weights()\n","        decoder_weights = self.decoder.get_weights()\n","        embedding_weights = self.embedding_layer.get_weights()\n","        return encoder_weights, decoder_weights, embedding_weights\n","\n","    def set_weights(self, weights):\n","        encoder_weights, decoder_weights, embedding_weights = weights\n","        self.encoder.set_weights(encoder_weights)\n","        self.decoder.set_weights(decoder_weights)\n","        self.embedding_layer.set_weights(embedding_weights)\n","    def compute_loss(self, targets, predictions):\n","        targets_flat = tf.reshape(targets, [-1])\n","        predictions_flat = tf.reshape(predictions, [-1, tf.shape(predictions)[-1]])\n","        targets_one_hot = tf.one_hot(targets_flat, depth=predictions_flat.shape[-1])\n","        print(f\"prediction shape before loss {predictions_flat.shape} , target shape before loss {targets_one_hot.shape}\")\n","        loss = tf.keras.losses.categorical_crossentropy(targets_one_hot, predictions_flat, from_logits=True)\n","        loss = tf.reduce_mean(loss)\n","        return loss\n","\n","    def train_step(self, eng_tokens, fr_tokens):\n","        eng_batch = self.embedding_layer(eng_tokens)\n","        fr_batch = self.embedding_layer(fr_tokens)\n","\n","        with tf.GradientTape() as tape:\n","            encoder_out = self.encoder(eng_batch)\n","            decoder_out = self.decoder(fr_batch, encoder_out)\n","            loss = self.compute_loss(fr_tokens[:, 1:], decoder_out[:, :-1])  # Ignore <BOS> token in targets\n","\n","        gradients = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n","        self.optimizer.apply_gradients(\n","            zip(gradients, self.encoder.trainable_variables + self.decoder.trainable_variables))\n","\n","        return loss\n","\n","    def fit(self, train_df, test_df, num_epochs, batch_size):\n","        train_english_sentences = train_df[\"English\"].values\n","        train_french_sentences = train_df[\"French\"].values\n","\n","        for epoch in range(num_epochs):\n","            epoch_loss = 0\n","            num_batches = len(train_english_sentences) // batch_size\n","            for i in range(1, int(train_english_sentences.shape[0] / batch_size)):\n","                eng = train_english_sentences[batch_size * i:batch_size * (i + 1)]\n","                fr = train_french_sentences[batch_size * i:batch_size * (i + 1)]\n","\n","                eng_token = np.array(tokenizer(list(eng), padding=True)['input_ids'])\n","                fr_token = np.array(tokenizer(list(fr), padding=True)['input_ids'])\n","                fr_token = np.insert(fr_token, 0, tokenizer.bos_token_id, axis=1)\n","\n","                loss = self.train_step(eng_token, fr_token)\n","                epoch_loss += loss\n","\n","                print(f\"Batch {i}/{num_batches} Loss: {loss:.4f} Epoch {epoch + 1}\")\n","\n","            model.save(f'weights_run2_{epoch}.pkl')\n","            print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / num_batches:.4f}\")\n","\n","            test_loss = self.evaluate(test_df, batch_size)\n","            print(f\"Validation Loss: {test_loss:.4f}\")\n","\n","    def evaluate(self, test_df, batch_size):\n","        test_english_sentences = test_df[\"English\"].values\n","        test_french_sentences = test_df[\"French\"].values\n","        total_loss = 0\n","\n","        num_batches = len(test_english_sentences) // batch_size\n","        for i in range(1, int(test_english_sentences.shape[0] / batch_size)):\n","            eng = test_english_sentences[batch_size * i:batch_size * (i + 1)]\n","            fr = test_french_sentences[batch_size * i:batch_size * (i + 1)]\n","\n","            eng_token = np.array(tokenizer(list(eng), padding=True)['input_ids'])\n","            fr_token = np.array(tokenizer(list(fr), padding=True)['input_ids'])\n","            fr_token = np.insert(fr_token, 0, tokenizer.bos_token_id, axis=1)\n","\n","            loss = self.compute_loss(fr_token[:, 1:], self.decoder(self.embedding_layer(fr_token),\n","                                                                   self.encoder(self.embedding_layer(eng_token)))[:,\n","                                                      :-1])\n","            total_loss += loss\n","\n","        return total_loss / num_batches\n","\n","    def inference(self, input_text, max_length=50):\n","        input_tokens = np.array(tokenizer([input_text], padding=True)['input_ids'])\n","\n","        output_tokens = np.array([[tokenizer.bos_token_id]])\n","\n","        for _ in range(max_length):\n","            input_embeddings = self.embedding_layer(input_tokens)\n","            output_embeddings = self.embedding_layer(output_tokens)\n","\n","            encoder_out = self.encoder(input_embeddings)\n","\n","            decoder_out = self.decoder(output_embeddings, encoder_out)\n","\n","            last_token_logits = decoder_out[:, -1, :]\n","\n","            next_token_id = tf.argmax(last_token_logits, axis=-1)\n","\n","            output_tokens = np.concatenate([output_tokens, next_token_id[:, tf.newaxis]], axis=-1)\n","\n","            if next_token_id[0] == tokenizer.eos_token_id:\n","                print('GOT EOS TOKEN')\n","                break\n","        output_text = tokenizer.decode(output_tokens[0])\n","\n","        return output_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Axhov_q_Bmg9"},"outputs":[],"source":["df = pd.read_csv(\"eng_fr.csv\", header=None, names=[\"English\", \"French\"])\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","embedding_layer = TransformerEmbedding(tokenizer.vocab_size + 1, 128, 60)\n","encoder = Encoder(dModel=128, num_blocks=6, num_heads=4)\n","decoder= Decoder(vocab_size=tokenizer.vocab_size + 1, dModel= 128, num_heads=4, num_blocks=3)\n","model = Transformer(embedding_layer, encoder, decoder,optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n","\n","#test...\n","\n","response=model.inference(\"Hello! How are you?\")\n","print(response)\n","\n","\n","model.fit(train_df, test_df, 3, )"]},{"cell_type":"code","source":["#Alternative Transformer class for question answering tasks\n","\n","class Transformer(tf.keras.Model):\n","    def __init__(self, embedding_layer, encoder, decoder, optimizer):\n","        super(Transformer, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.embedding_layer = embedding_layer\n","        self.optimizer = optimizer\n","\n","    def save(self, filepath):\n","        weights = self.get_weights()\n","        with open(filepath, 'wb') as f:\n","            pickle.dump(weights, f)\n","\n","    def load(self, filepath):\n","        with open(filepath, 'rb') as f:\n","            weights = pickle.load(f)\n","        self.set_weights(weights)\n","\n","    def get_weights(self):\n","        encoder_weights = self.encoder.get_weights()\n","        decoder_weights = self.decoder.get_weights()\n","        embedding_weights = self.embedding_layer.get_weights()\n","        return encoder_weights, decoder_weights, embedding_weights\n","\n","    def set_weights(self, weights):\n","        encoder_weights, decoder_weights, embedding_weights = weights\n","        self.encoder.set_weights(encoder_weights)\n","        self.decoder.set_weights(decoder_weights)\n","        self.embedding_layer.set_weights(embedding_weights)\n","\n","    def compute_loss(self, targets, predictions):\n","\n","\n","        max_length = max(tf.shape(targets)[1], tf.shape(predictions)[1])\n","        pad_targets = tf.pad(targets, [[0, 0], [0, max_length - tf.shape(targets)[1]]])\n","        pad_predictions = tf.pad(predictions, [[0, 0], [0, max_length - tf.shape(predictions)[1]], [0, 0]])\n","\n","\n","\n","        targets_flat = tf.reshape(pad_targets, [-1])\n","        predictions_flat = tf.reshape(pad_predictions, [-1, tf.shape(pad_predictions)[-1]])\n","        targets_one_hot = tf.one_hot(targets_flat, depth=predictions_flat.shape[-1])\n","        loss = tf.keras.losses.categorical_crossentropy(targets_one_hot, predictions_flat, from_logits=True)\n","        loss = tf.reduce_mean(loss)\n","        return loss\n","\n","\n","    def train_step(self, context_tokens, question_tokens, answer_tokens):\n","        context_batch = self.embedding_layer(context_tokens)\n","        question_batch = self.embedding_layer(question_tokens)\n","\n","        with tf.GradientTape() as tape:\n","            encoder_out = self.encoder(context_batch)\n","            decoder_out = self.decoder(question_batch, encoder_out)\n","\n","            loss = self.compute_loss(answer_tokens[:, 1:], decoder_out[:, :-1])  # Ignore <BOS> token in targets\n","\n","        gradients = tape.gradient(loss, self.encoder.trainable_variables + self.decoder.trainable_variables)\n","        self.optimizer.apply_gradients(\n","            zip(gradients, self.encoder.trainable_variables + self.decoder.trainable_variables))\n","\n","        return loss\n","\n","    def fit(self, train_data, test_data, num_epochs, batch_size):\n","        num_batches = len(train_data['context']) // batch_size\n","        for epoch in range(num_epochs):\n","            epoch_loss = 0\n","            for i in range(num_batches):\n","                context_batch = train_data['context'][i * batch_size:(i + 1) * batch_size]\n","                question_batch = train_data['question'][i * batch_size:(i + 1) * batch_size]\n","                answer_batch = train_data['answer'][i * batch_size:(i + 1) * batch_size]\n","\n","                context_tokens = np.array(tokenizer(list(context_batch), padding=True)['input_ids'])\n","                question_tokens = np.array(tokenizer(list(question_batch), padding=True)['input_ids'])\n","                answer_tokens = np.array(tokenizer(list(answer_batch), padding=True)['input_ids'])\n","                answer_tokens = np.insert(answer_tokens, 0, tokenizer.bos_token_id, axis=1)\n","\n","                loss = self.train_step(context_tokens, question_tokens, answer_tokens)\n","                epoch_loss += loss\n","\n","                print(f\"Batch {i + 1}/{num_batches} Loss: {loss:.4f} Epoch {epoch + 1}\")\n","\n","            avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else epoch_loss\n","            self.save(f'weights_run2_{epoch}.pkl')\n","            print(f\"Epoch {epoch + 1}, Loss: {avg_epoch_loss:.4f}\")\n","\n","            test_loss = self.evaluate(test_data, batch_size)\n","            print(f\"Validation Loss: {test_loss:.4f}\")\n","\n","    def evaluate(self, test_data, batch_size):\n","        total_loss = 0\n","        num_batches = len(test_data) // batch_size\n","        for i in range(num_batches):\n","            context_batch = test_data[i * batch_size:(i + 1) * batch_size]['context']\n","            question_batch = test_data[i * batch_size:(i + 1) * batch_size]['question']\n","            answer_batch = test_data[i * batch_size:(i + 1) * batch_size]['answer']\n","\n","            context_tokens = np.array(tokenizer(list(context_batch), padding=True)['input_ids'])\n","            question_tokens = np.array(tokenizer(list(question_batch), padding=True)['input_ids'])\n","            answer_tokens = np.array(tokenizer(list(answer_batch), padding=True)['input_ids'])\n","            answer_tokens = np.insert(answer_tokens, 0, tokenizer.bos_token_id, axis=1)\n","\n","            loss = self.compute_loss(answer_tokens[:, 1:], self.decoder(self.embedding_layer(answer_tokens),\n","                                                                       self.encoder(self.embedding_layer(context_tokens)))[:, :-1])\n","            total_loss += loss\n","        if(num_batches==0):\n","          num_batches=1\n","        return total_loss / 1\n","\n","    def inference(self, context_text, question_text, max_length=50):\n","        context_tokens = np.array(tokenizer([context_text], padding=True)['input_ids'])\n","        question_tokens = np.array(tokenizer([question_text], padding=True)['input_ids'])\n","\n","        output_tokens = np.array([[tokenizer.bos_token_id]])\n","\n","        for _ in range(max_length):\n","            context_embeddings = self.embedding_layer(context_tokens)\n","            question_embeddings = self.embedding_layer(question_tokens)\n","            output_embeddings = self.embedding_layer(output_tokens)\n","\n","            encoder_out = self.encoder(context_embeddings)\n","\n","            decoder_out = self.decoder(output_embeddings, encoder_out)\n","\n","            last_token_logits = decoder_out[:, -1, :]\n","\n","            next_token_id = tf.argmax(last_token_logits, axis=-1)\n","\n","            output_tokens = np.concatenate([output_tokens, next_token_id[:, tf.newaxis]], axis=-1)\n","\n","            if next_token_id[0] == tokenizer.eos_token_id:\n","                print('GOT EOS TOKEN')\n","                break\n","\n","        output_text = tokenizer.decode(output_tokens[0])\n","        return output_text"],"metadata":{"id":"nuM4W32rU1Xx"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyM0xANMjLjLWtBtgrRJsSHo"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}